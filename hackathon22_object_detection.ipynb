{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import glob\n",
    "import heapq\n",
    "import json\n",
    "import numpy\n",
    "import os\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import tensorflow as tf\n",
    "import uuid\n",
    "\n",
    "# %pylab inline\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, \"/recsys_data/RecSys/fashion/automl/efficientnetv2\")\n",
    "import effnetv2_model\n",
    "\n",
    "from data_process import OutfitGen, OutfitGenWithImage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import numpy as np\n",
    "import tempfile\n",
    "import shutil\n",
    "\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from pytorchyolo.models import load_model\n",
    "from pytorchyolo.utils.utils import load_classes, rescale_boxes, non_max_suppression, print_environment_info\n",
    "from pytorchyolo.utils.datasets import ImageFolder\n",
    "from pytorchyolo.utils.transforms import Resize, DEFAULT_TRANSFORMS\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "from matplotlib.ticker import NullLocator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import anvil.server\n",
    "import anvil.media\n",
    "import anvil.mpl_util\n",
    "\n",
    "anvil.server.connect(\"D3BJ3YVOSVMXFCEVTGLVI3XH-5GA6BUC4BEGN43RS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = \"/recsys_data/RecSys/fashion/polyvore-dataset/polyvore_outfits\"\n",
    "data_type = \"disjoint\" # \"nondisjoint\"\n",
    "train_dir = os.path.join(base_dir, data_type)\n",
    "image_dir = os.path.join(base_dir, \"images\")\n",
    "embed_dir = \"/recsys_data/RecSys/fashion/polyvore-dataset/precomputed\"\n",
    "model_name = \"efficientnet\"\n",
    "checkpoint_filepath = base_dir + \"/checkpoint\"\n",
    "image_embedding_file = os.path.join(embed_dir, \"effnet_tuned_polyvore.pkl\")\n",
    "data_type = \"nondisjoint\"\n",
    "model_type = \"rnn\"\n",
    "max_seq_len = 8\n",
    "item_file = \"polyvore_item_metadata.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect(model, dataloader, output_path, conf_thres, nms_thres):\n",
    "    \"\"\"Inferences images with model.\n",
    "\n",
    "    :param model: Model for inference\n",
    "    :type model: models.Darknet\n",
    "    :param dataloader: Dataloader provides the batches of images to inference\n",
    "    :type dataloader: DataLoader\n",
    "    :param output_path: Path to output directory\n",
    "    :type output_path: str\n",
    "    :param conf_thres: Object confidence threshold, defaults to 0.5\n",
    "    :type conf_thres: float, optional\n",
    "    :param nms_thres: IOU threshold for non-maximum suppression, defaults to 0.5\n",
    "    :type nms_thres: float, optional\n",
    "    :return: List of detections. The coordinates are given for the padded image that is provided by the dataloader.\n",
    "        Use `utils.rescale_boxes` to transform them into the desired input image coordinate system before its transformed by the dataloader),\n",
    "        List of input image paths\n",
    "    :rtype: [Tensor], [str]\n",
    "    \"\"\"\n",
    "    # Create output directory, if missing\n",
    "    os.makedirs(output_path, exist_ok=True)\n",
    "\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "\n",
    "    Tensor = torch.cuda.FloatTensor if torch.cuda.is_available() else torch.FloatTensor\n",
    "\n",
    "    img_detections = []  # Stores detections for each image index\n",
    "    imgs = []  # Stores image paths\n",
    "\n",
    "    for (img_paths, input_imgs) in tqdm(dataloader, desc=\"Detecting\"):\n",
    "        # Configure input\n",
    "        input_imgs = Variable(input_imgs.type(Tensor))\n",
    "\n",
    "        # Get detections\n",
    "        with torch.no_grad():\n",
    "            detections = model(input_imgs)\n",
    "            print(detections.shape)\n",
    "            detections = non_max_suppression(detections, conf_thres, nms_thres)\n",
    "            print(detections)\n",
    "        # Store image and detections\n",
    "        img_detections.extend(detections)\n",
    "        imgs.extend(img_paths)\n",
    "    return img_detections, imgs\n",
    "\n",
    "\n",
    "def _draw_and_save_output_images(img_detections, imgs, img_size, output_path, classes):\n",
    "    \"\"\"Draws detections in output images and stores them.\n",
    "\n",
    "    :param img_detections: List of detections\n",
    "    :type img_detections: [Tensor]\n",
    "    :param imgs: List of paths to image files\n",
    "    :type imgs: [str]\n",
    "    :param img_size: Size of each image dimension for yolo\n",
    "    :type img_size: int\n",
    "    :param output_path: Path of output directory\n",
    "    :type output_path: str\n",
    "    :param classes: List of class names\n",
    "    :type classes: [str]\n",
    "    \"\"\"\n",
    "\n",
    "    # Iterate through images and save plot of detections\n",
    "    outpaths, detcns = [], []\n",
    "    for (image_path, detections) in zip(imgs, img_detections):\n",
    "        print(f\"Image {image_path}:\")\n",
    "        op, dtcn = _draw_and_save_output_image(\n",
    "            image_path, detections, img_size, output_path, classes)\n",
    "        outpaths.append(op)\n",
    "        detcns.append(dtcn)\n",
    "    return outpaths, detcns\n",
    "\n",
    "\n",
    "def _draw_and_save_output_image(image_path, detections, img_size, output_path, classes):\n",
    "    \"\"\"Draws detections in output image and stores this.\n",
    "\n",
    "    :param image_path: Path to input image\n",
    "    :type image_path: str\n",
    "    :param detections: List of detections on image\n",
    "    :type detections: [Tensor]\n",
    "    :param img_size: Size of each image dimension for yolo\n",
    "    :type img_size: int\n",
    "    :param output_path: Path of output directory\n",
    "    :type output_path: str\n",
    "    :param classes: List of class names\n",
    "    :type classes: [str]\n",
    "    \"\"\"\n",
    "    # Create plot\n",
    "    img = np.array(Image.open(image_path))\n",
    "    plt.figure()\n",
    "    fig, ax = plt.subplots(1)\n",
    "    ax.imshow(img)\n",
    "    # Rescale boxes to original image\n",
    "    detections = rescale_boxes(detections, img_size, img.shape[:2])\n",
    "    unique_labels = detections[:, -1].cpu().unique()\n",
    "    n_cls_preds = len(unique_labels)\n",
    "\n",
    "    # Bounding-box colors\n",
    "    cmap = plt.get_cmap(\"tab20b\")\n",
    "    colors = [cmap(i) for i in np.linspace(0, 1, n_cls_preds)]\n",
    "    bbox_colors = random.sample(colors, n_cls_preds)\n",
    "    for x1, y1, x2, y2, conf, cls_pred in detections:\n",
    "\n",
    "        print(f\"\\t+ Label: {classes[int(cls_pred)]} | Confidence: {conf.item():0.4f}\")\n",
    "        print(x1, y1, x2, y2)\n",
    "\n",
    "        box_w = x2 - x1\n",
    "        box_h = y2 - y1\n",
    "\n",
    "        color = bbox_colors[int(np.where(unique_labels == int(cls_pred))[0])]\n",
    "        # Create a Rectangle patch\n",
    "        bbox = patches.Rectangle((x1, y1), box_w, box_h, linewidth=2, edgecolor=color, facecolor=\"none\")\n",
    "        # Add the bbox to the plot\n",
    "        ax.add_patch(bbox)\n",
    "        # Add label\n",
    "        plt.text(\n",
    "            x1,\n",
    "            y1,\n",
    "            s=classes[int(cls_pred)],\n",
    "            color=\"white\",\n",
    "            verticalalignment=\"top\",\n",
    "            bbox={\"color\": color, \"pad\": 0})\n",
    "\n",
    "    # Save generated image with detections\n",
    "    plt.axis(\"off\")\n",
    "    plt.gca().xaxis.set_major_locator(NullLocator())\n",
    "    plt.gca().yaxis.set_major_locator(NullLocator())\n",
    "    \n",
    "    filename = os.path.basename(image_path).split(\".\")[0]\n",
    "    output_path = os.path.join(output_path, f\"{filename}.png\")\n",
    "    \n",
    "    plt.savefig(output_path, bbox_inches=\"tight\", pad_inches=0.0)\n",
    "    plt.close()\n",
    "    return output_path, detections\n",
    "\n",
    "def _create_data_loader(img_path, batch_size, img_size, n_cpu):\n",
    "    \"\"\"Creates a DataLoader for inferencing.\n",
    "\n",
    "    :param img_path: Path to file containing all paths to validation images.\n",
    "    :type img_path: str\n",
    "    :param batch_size: Size of each image batch\n",
    "    :type batch_size: int\n",
    "    :param img_size: Size of each image dimension for yolo\n",
    "    :type img_size: int\n",
    "    :param n_cpu: Number of cpu threads to use during batch generation\n",
    "    :type n_cpu: int\n",
    "    :return: Returns DataLoader\n",
    "    :rtype: DataLoader\n",
    "    \"\"\"\n",
    "    dataset = ImageFolder(\n",
    "        img_path,\n",
    "        transform=transforms.Compose([DEFAULT_TRANSFORMS, Resize(img_size)]))\n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=n_cpu,\n",
    "        pin_memory=True)\n",
    "    return dataloader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ModaNet\n",
    "model_cfg = \"/recsys_data/RecSys/fashion/Clothing-Detection/yolo/modanetcfg/yolov3-modanet.cfg\"\n",
    "model_wts = \"/recsys_data/RecSys/fashion/Clothing-Detection/yolo/weights/fashion_models/yolov3-modanet_last.weights\"\n",
    "class_names = \"/recsys_data/RecSys/fashion/Clothing-Detection/yolo/modanetcfg/modanet.names\"\n",
    "\n",
    "# DeepFashion-2\n",
    "# model_cfg = \"/recsys_data/RecSys/PyTorch-YOLOv3/config/yolov3-custom.cfg\"\n",
    "# model_wts = \"/recsys_data/RecSys/PyTorch-YOLOv3/checkpoints/yolov3_ckpt_2.pth\"\n",
    "# class_names = \"/recsys_data/RecSys/PyTorch-YOLOv3/data/custom/classes.names\"\n",
    "\n",
    "img_size = 416\n",
    "\n",
    "classes = load_classes(class_names)\n",
    "model = load_model(model_cfg, model_wts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eff2_model = tf.keras.models.Sequential(\n",
    "                [\n",
    "                    tf.keras.layers.InputLayer(input_shape=[224, 224, 3]),\n",
    "                    effnetv2_model.get_model(\"efficientnetv2-b0\", include_top=False),\n",
    "                ]\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_similar_products_from_array(image_arr, categories=None):\n",
    "    \"\"\"\n",
    "    Search for similar items only in these categories\n",
    "    \"\"\"\n",
    "    if categories is not None:\n",
    "        search_products = set()\n",
    "        for cat in categories:\n",
    "            search_products |= set(cat2items[cat])\n",
    "    else:\n",
    "        search_products = image_embedding_dict.keys()\n",
    "        \n",
    "    image_arr = tf.image.resize_with_pad(image_arr, target_height=224, target_width=224).numpy()\n",
    "    image_arr /= 255.0\n",
    "    image_embed = tf.squeeze(eff2_model(tf.expand_dims(image_arr, 0)))\n",
    "\n",
    "    q_vec = image_embed #.numpy()\n",
    "    image_dist = []\n",
    "    count = 0\n",
    "    for image in tqdm(search_products):\n",
    "        i_vec = image_embedding_dict[image]\n",
    "        dist = numpy.linalg.norm(q_vec - i_vec.numpy())\n",
    "        heapq.heappush(image_dist, (dist, image))\n",
    "        count += 1\n",
    "\n",
    "    top_images = [heapq.heappop(image_dist) for _ in range(5)]\n",
    "#     top_images = [item for s, item in top_images]\n",
    "#     top_images = [os.path.join(image_dir, item + \".jpg\") for s, item in top_images]\n",
    "#     top_images = [ anvil.media.from_file(os.path.join(image_dir, item + \".jpg\"), 'image/jpeg') for s, item in top_images]\n",
    "    return top_images\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(image_embedding_file, \"rb\") as fr:\n",
    "    image_embedding_dict = pickle.load(fr)\n",
    "print(f\"Loaded {len(image_embedding_dict)} image embeddings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modanet2polyvore = {'bag': 'bags',\n",
    "                    'belt': 'accessories',\n",
    "                    'boots': 'shoes',\n",
    "                    'footwear': 'shoes',\n",
    "                    'outer': 'outerwear',\n",
    "                    'dress': 'all-body',\n",
    "                    'sunglasses': 'sunglasses',\n",
    "                    'pants': 'bottoms',\n",
    "                    'top': 'tops',\n",
    "                    'shorts': 'bottoms',\n",
    "                    'skirt': 'bottoms',\n",
    "                    'headwear': 'hats',\n",
    "                    'scarf/tie': 'scarves',\n",
    "                   }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(base_dir, item_file), 'r') as fr:\n",
    "    pv_items = json.load(fr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2cat = dict()\n",
    "cat2items = dict()  # seggregate items by category\n",
    "all_scats = list()\n",
    "for item in pv_items:\n",
    "    iid, cat = pv_items[item][\"category_id\"], pv_items[item][\"semantic_category\"]\n",
    "    if iid not in id2cat:\n",
    "        id2cat[iid] = cat\n",
    "    if cat not in cat2items:\n",
    "        cat2items[cat] = []\n",
    "    cat2items[cat].append(item)\n",
    "    all_scats.append(cat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shop The Look"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@anvil.server.callable\n",
    "def get_fashion_objects(file):\n",
    "    \"\"\"\n",
    "    dependencies:\n",
    "        get_similar_products_from_array()\n",
    "        modanet2polyvore dict\n",
    "        eff2_model\n",
    "    \"\"\"\n",
    "    img_size = 416\n",
    "    temp_dir = tempfile.TemporaryDirectory()\n",
    "    output_path = tempfile.TemporaryDirectory()\n",
    "    img_path = temp_dir.name+\"/test.jpg\"\n",
    "    with anvil.media.TempFile(file) as filename:\n",
    "        shutil.copyfile(filename, img_path)\n",
    "\n",
    "    dataloader = _create_data_loader(temp_dir.name, batch_size=1, img_size=416, n_cpu=6)\n",
    "    img_detections, imgs = detect(\n",
    "        model,\n",
    "        dataloader,\n",
    "        output_path.name,\n",
    "        conf_thres=0.5,\n",
    "        nms_thres=0.4)\n",
    "\n",
    "    filepaths, details = _draw_and_save_output_images(\n",
    "        img_detections, imgs, img_size, output_path.name, classes)\n",
    "    \n",
    "    res = {'original': []}\n",
    "    for filepath in filepaths:\n",
    "        res['original'].append(anvil.media.from_file(filepath, 'image/png'))\n",
    "    \n",
    "    # original file\n",
    "    im = np.array(Image.open(img_path))\n",
    "    for x1, y1, x2, y2, conf, cls_pred in details[0]:\n",
    "        pred_class = classes[int(cls_pred)]\n",
    "        if pred_class in (\"outer\", \"dress\", \"pants\", \"top\", \"shorts\", \"skirt\"):\n",
    "            res[pred_class] = []\n",
    "            A = im[int(y1):int(y2), int(x1):int(x2), :]\n",
    "            im2 = Image.fromarray(A)\n",
    "            bfile = temp_dir.name+f\"/{pred_class}.png\"\n",
    "            im2.save(bfile)\n",
    "            res[pred_class].append(anvil.media.from_file(bfile, 'image/png'))\n",
    "            similar_prods = get_similar_products_from_array(image_arr=A, \n",
    "                                                            categories=[modanet2polyvore[pred_class]])\n",
    "            top_prods = [anvil.media.from_file(os.path.join(image_dir, item + \".jpg\"), 'image/jpeg') for s, item in similar_prods]\n",
    "            res[pred_class].extend(top_prods)\n",
    "\n",
    "    temp_dir.cleanup()\n",
    "    output_path.cleanup()\n",
    "\n",
    "    return res\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Outfit Compatibility Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from focal_loss import BinaryFocalLoss\n",
    "\n",
    "compatibility_model = tf.keras.models.load_model('compatibility_nondisjoint_single-transformer_model_8_only_image')\n",
    "compatibility_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_top_items(query_item, model, image_embedding_dict, item_dict, ignore=None, search_only=None, max_item=8):\n",
    "    if type(query_item) is not list:\n",
    "        query_item = [query_item]\n",
    "    data_gen = OutfitGenWithImage(embed_dir=embed_dir,\n",
    "                                  text_embed_file=\"bert_polyvore.pkl\",\n",
    "                                  batch_size=256,\n",
    "                                  max_len=max_item,\n",
    "                                  image_embedding_dim=1280,\n",
    "                                  query_item=query_item,\n",
    "                                  image_embedding_dict=image_embedding_dict,\n",
    "                                  item_description=item_dict,\n",
    "                                  item_category_dict=cat2items,\n",
    "                                  # ignore_categories=ignore,\n",
    "                                  search_only_categories=search_only,\n",
    "                                 )\n",
    "    pbar = tqdm(range(len(data_gen)))\n",
    "    current_score = []\n",
    "    # adds one more item to the list of current items (stored in query_item)\n",
    "    for ii in pbar:\n",
    "        x, items = data_gen[ii]\n",
    "        yhat = model(x)\n",
    "        for cs, item in zip(yhat, items):\n",
    "            heapq.heappush(current_score, (1-cs, query_item + [str(item)]))  # it's a min-heap\n",
    "    return current_score\n",
    "\n",
    "def filter_outfits(outfits, item_dict, max_len):\n",
    "    # retain only those combinations where all categories are distinct\n",
    "    count = 0\n",
    "    filtered = []\n",
    "    while count < max_len: \n",
    "        outfit = heapq.heappop(outfits)\n",
    "        items = outfit[1]\n",
    "        categories = [item_dict[item]['semantic_category'] for item in items]\n",
    "        if len(set(categories)) == len(categories):\n",
    "            filtered.append(items)\n",
    "            count += 1\n",
    "    return filtered\n",
    "\n",
    "def get_next_category(cats, global_categories):\n",
    "    # returns the next category to search from\n",
    "    for cat in global_categories:\n",
    "        if cat not in cats:\n",
    "            return cat\n",
    "    return None\n",
    "\n",
    "def create_outfit(query, category, model, image_dict, item_dict, max_item=8, beam_length=2):\n",
    "    \"\"\"\n",
    "        query: image embedding or polyvore item-id\n",
    "        catgeory: category of the image, str\n",
    "        model: compatibility model (pretrained)\n",
    "        image_dict: image to embedding dict\n",
    "        item_dict: item to item details dict\n",
    "        max_item: maximum number of items in the outfit\n",
    "        beam_length: number of alternative outfits\n",
    "    \"\"\"\n",
    "    ## TODO: modify for multiple image inputs \n",
    "    \n",
    "    # create new item-id and update embedding_dict and category_dict\n",
    "    if type(query) is not str:\n",
    "        item_id = str(uuid.uuid4())\n",
    "        image_dict[item_id] = query\n",
    "        item_dict[item_id] = {}\n",
    "        item_dict[item_id]['semantic_category'] = category\n",
    "    else:\n",
    "        item_id = query\n",
    "    \n",
    "    # Based on the global category distribution find the next category to search from\n",
    "    top_level_category_order = ['shoes', 'jewellery', 'bags', 'tops', 'bottoms',\n",
    "                      'all-body', 'outerwear', 'sunglasses', 'accessories', 'hats', 'scarves']\n",
    "    next_category = get_next_category([category], top_level_category_order)\n",
    "\n",
    "    # add the first item - only one run\n",
    "    first_score = return_top_items(item_id,\n",
    "                                   model,\n",
    "                                   image_dict, \n",
    "                                   item_dict, \n",
    "                                   ignore=[category], \n",
    "                                   search_only=[next_category])\n",
    "    current_items = filter_outfits(first_score, item_dict, beam_length)\n",
    "    \n",
    "    for jj in range(1, max_item):\n",
    "        all_scores = []\n",
    "        for ii in range(beam_length):\n",
    "            ignore_categories = [item_dict[item]['semantic_category'] for item in current_items[ii]]\n",
    "            next_category = get_next_category(ignore_categories, top_level_category_order)\n",
    "            scores_ii = return_top_items(current_items[ii],\n",
    "                                         model,\n",
    "                                         image_dict, \n",
    "                                         item_dict, \n",
    "                                         ignore_categories,\n",
    "                                         search_only=[next_category],\n",
    "                                        )\n",
    "            all_scores += scores_ii\n",
    "        # reconstruct current items - with one new item\n",
    "        current_items = filter_outfits(all_scores, item_dict, beam_length)\n",
    "    \n",
    "    return current_items\n",
    "\n",
    "def get_complementary_products_from_array(image_arr, image_cat, max_item=4):\n",
    "    \"\"\"\n",
    "    Search for complementary items using Compatibility model\n",
    "    \"\"\"\n",
    "    image_arr = tf.image.resize_with_pad(image_arr, target_height=224, target_width=224).numpy()\n",
    "    image_arr /= 255.0\n",
    "    image_embed = tf.squeeze(eff2_model(tf.expand_dims(image_arr, 0)))\n",
    "\n",
    "    outfits = create_outfit(query=image_embed,\n",
    "                            category=image_cat,\n",
    "                            model=compatibility_model,\n",
    "                            image_dict=image_embedding_dict.copy(), \n",
    "                            item_dict=pv_items.copy(), \n",
    "                            max_item=int(max_item), \n",
    "                            beam_length=1)\n",
    "    # since beam_length is fixed at 1, we take the first element\n",
    "    # also the first element in the list is the original item that\n",
    "    # we don't have to return\n",
    "    return outfits[0][1:]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complete The Look"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@anvil.server.callable\n",
    "def complete_the_look(file):\n",
    "    \"\"\"\n",
    "    dependencies:\n",
    "        get_similar_products_from_array()\n",
    "        modanet2polyvore dict\n",
    "        eff2_model\n",
    "        compatibility model\n",
    "    \"\"\"\n",
    "    img_size = 416\n",
    "    temp_dir = tempfile.TemporaryDirectory()\n",
    "    output_path = tempfile.TemporaryDirectory()\n",
    "    img_path = temp_dir.name+\"/test.jpg\"\n",
    "    with anvil.media.TempFile(file) as filename:\n",
    "        shutil.copyfile(filename, img_path)\n",
    "\n",
    "    dataloader = _create_data_loader(temp_dir.name, batch_size=1, img_size=416, n_cpu=6)\n",
    "    img_detections, imgs = detect(\n",
    "        model,\n",
    "        dataloader,\n",
    "        output_path.name,\n",
    "        conf_thres=0.5,\n",
    "        nms_thres=0.4)\n",
    "\n",
    "    filepaths, details = _draw_and_save_output_images(\n",
    "        img_detections, imgs, img_size, output_path.name, classes)\n",
    "    \n",
    "    res = {'original': []}\n",
    "    for filepath in filepaths:\n",
    "        res['original'].append(anvil.media.from_file(filepath, 'image/png'))\n",
    "    \n",
    "    # original file\n",
    "    im = np.array(Image.open(img_path))\n",
    "    for x1, y1, x2, y2, conf, cls_pred in details[0]:\n",
    "        pred_class = classes[int(cls_pred)]\n",
    "        if pred_class in (\"outer\", \"dress\", \"pants\", \"top\", \"shorts\", \"skirt\"):\n",
    "            res[pred_class] = []\n",
    "            A = im[int(y1):int(y2), int(x1):int(x2), :]\n",
    "            im2 = Image.fromarray(A)\n",
    "            bfile = temp_dir.name+f\"/{pred_class}.png\"\n",
    "            im2.save(bfile)\n",
    "            res[pred_class].append(anvil.media.from_file(bfile, 'image/png'))\n",
    "            complementary_prods = get_complementary_products_from_array(image_arr=A, \n",
    "                                                            image_cat=modanet2polyvore[pred_class])\n",
    "            print(complementary_prods)\n",
    "            top_prods = [anvil.media.from_file(os.path.join(image_dir, item + \".jpg\"), 'image/jpeg') for item in complementary_prods]\n",
    "            res[pred_class].extend(top_prods)\n",
    "\n",
    "    temp_dir.cleanup()\n",
    "    output_path.cleanup()\n",
    "\n",
    "    return res\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outfit Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_model = tf.keras.models.load_model(f\"finetuned_efficientnet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# original logic of creating the labels\n",
    "# taken from ImageDataGen()\n",
    "X, y = [], []\n",
    "for item_id in pv_items:\n",
    "    X.append(item_id)\n",
    "    y.append(pv_items[item_id][\"category_id\"])\n",
    "\n",
    "X_col = \"X\"\n",
    "y_col = \"y\"\n",
    "df = pd.DataFrame({X_col: X, y_col: y})\n",
    "categories = df[y_col].unique()\n",
    "label_dict = {ii: jj for ii, jj in enumerate(categories)} # reversing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_product_category(query):\n",
    "    image = tf.keras.preprocessing.image.load_img(query)\n",
    "    image_arr = tf.keras.preprocessing.image.img_to_array(image)\n",
    "    # image_arr = tf.image.resize(image_arr, (224, 224)).numpy()\n",
    "    image_arr = tf.image.resize_with_pad(image_arr, target_height=224, target_width=224).numpy()\n",
    "    image_arr /= 255.0\n",
    "    pred_score = category_model(tf.expand_dims(image_arr, 0))\n",
    "    pred_label = np.argmax(pred_score, axis=1)[0]\n",
    "    pred_label = label_dict[pred_label]  # class number as given in the item description\n",
    "    return pred_label, id2cat[pred_label]\n",
    "\n",
    "def get_product_category_2(image_arr):\n",
    "    pred_score = category_model(tf.expand_dims(image_arr, 0))\n",
    "    pred_label = np.argmax(pred_score, axis=1)[0]  # index of the class\n",
    "    pred_label = label_dict[pred_label]  # class number as given in the item description\n",
    "    return pred_label, id2cat[pred_label]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@anvil.server.callable\n",
    "def get_outfit(file, max_item=3):\n",
    "    with anvil.media.TempFile(file) as filename:\n",
    "        image = tf.keras.preprocessing.image.load_img(filename)\n",
    "\n",
    "    image_arr = tf.keras.preprocessing.image.img_to_array(image)\n",
    "    # image_arr = tf.image.resize(image_arr, (224, 224)).numpy()\n",
    "    image_arr = tf.image.resize_with_pad(image_arr, target_height=224, target_width=224).numpy()\n",
    "    image_arr /= 255.0\n",
    "    image_embed = tf.squeeze(eff2_model(tf.expand_dims(image_arr, 0)))\n",
    "    image_cat_id, image_cat = get_product_category_2(image_arr)\n",
    "    print(image_cat_id, image_cat)\n",
    "    outfits = create_outfit(query=image_embed,\n",
    "                            category=image_cat,\n",
    "                            model=compatibility_model,\n",
    "                            image_dict=image_embedding_dict.copy(), \n",
    "                            item_dict=pv_items.copy(), \n",
    "                            max_item=int(max_item), \n",
    "                            beam_length=2)\n",
    "    print(outfits)\n",
    "    outfit_images = []\n",
    "    for prods in outfits:\n",
    "        res = []\n",
    "        for item in prods[1:]:\n",
    "            filepath = os.path.join(image_dir, item + \".jpg\")\n",
    "            res.append(anvil.media.from_file(filepath, 'image/jpeg'))\n",
    "        outfit_images.append(res)\n",
    "    return outfit_images\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Search Similar Products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@anvil.server.callable\n",
    "def get_similar_products(file):\n",
    "    with anvil.media.TempFile(file) as filename:\n",
    "        image = tf.keras.preprocessing.image.load_img(filename)\n",
    "\n",
    "    image_arr = tf.keras.preprocessing.image.img_to_array(image)\n",
    "    # image_arr = tf.image.resize(image_arr, (224, 224)).numpy()\n",
    "    image_arr = tf.image.resize_with_pad(image_arr, target_height=224, target_width=224).numpy()\n",
    "    image_arr /= 255.0\n",
    "    cid, cat = get_product_category_2(image_arr)\n",
    "    search_products = cat2items[cat]\n",
    "    print(f\"Searching in {cat} category, {len(search_products)} products\")\n",
    "    \n",
    "    image_embed = tf.squeeze(eff2_model(tf.expand_dims(image_arr, 0)))\n",
    "    q_vec = image_embed #.numpy()\n",
    "    image_dist = []\n",
    "    count = 0\n",
    "    for image in tqdm(search_products):\n",
    "        i_vec = image_embedding_dict[image]\n",
    "#     for image, i_vec in tqdm(image_embedding_dict.items()):\n",
    "        dist = numpy.linalg.norm(q_vec - i_vec.numpy())\n",
    "        heapq.heappush(image_dist, (dist, image))\n",
    "        count += 1\n",
    "\n",
    "    top_images = [heapq.heappop(image_dist) for _ in range(5)]\n",
    "    top_images = [ anvil.media.from_file(os.path.join(image_dir, item + \".jpg\"), 'image/jpeg') for s, item in top_images]\n",
    "    return top_images\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outfit Compatibility Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@anvil.server.callable\n",
    "def get_compatibility_score(files):\n",
    "    size = 224, 224\n",
    "    embed_dim = 1280\n",
    "    max_len = 8\n",
    "    outfit_embeddings = []\n",
    "    count = 0\n",
    "    zero_elem_image = np.zeros(embed_dim)\n",
    "    for file in files:\n",
    "        with anvil.media.TempFile(file) as filename:\n",
    "            image = tf.keras.preprocessing.image.load_img(filename)\n",
    "            image_arr = tf.keras.preprocessing.image.img_to_array(image)\n",
    "            w, h, c = image_arr.shape\n",
    "            aspect = h*1.0/w\n",
    "            if aspect > 1.5:\n",
    "                image_arr = tf.image.resize_with_pad(image_arr, target_height=224, target_width=224).numpy()\n",
    "            else:\n",
    "                image_arr = tf.image.resize(image_arr, size).numpy()\n",
    "            image_arr /= 255.0\n",
    "            image_embed = tf.squeeze(eff2_model(tf.expand_dims(image_arr, 0)))\n",
    "            outfit_embeddings.append(image_embed)\n",
    "            count += 1\n",
    "\n",
    "    zeros_image = [zero_elem_image for _ in range(max_len - count)]\n",
    "    outfit_embeddings = np.stack(zeros_image + outfit_embeddings)\n",
    "    outfit_embeddings = np.expand_dims(outfit_embeddings, 0)\n",
    "    score = compatibility_model(outfit_embeddings).numpy()[0][0]\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_outfit_2(query, category, required, model, image_dict, item_dict, beam_length=10):\n",
    "    # query is always a list of images\n",
    "    # create new item-id and update embedding_dict and category_dict\n",
    "    # get only one item\n",
    "    items = []\n",
    "    for item, cat in zip(query, category):\n",
    "        item_id = str(uuid.uuid4())\n",
    "        image_dict[item_id] = item\n",
    "        item_dict[item_id] = {}\n",
    "        item_dict[item_id]['semantic_category'] = cat\n",
    "        items.append(item_id)\n",
    "    \n",
    "    # add the first item - only one run\n",
    "    if required is not None:\n",
    "        print(required)\n",
    "        all_cats = set(list(id2cat.values()))\n",
    "        category = all_cats.difference(set(required))\n",
    "        \n",
    "    first_score = return_top_items(items, \n",
    "                                   model, \n",
    "                                   image_dict, \n",
    "                                   item_dict, \n",
    "                                   ignore=category,\n",
    "                                   search_only=required\n",
    "                                  )\n",
    "    if len(category) == len(set(category)):\n",
    "        # if the original outfit has duplicate categories then the following\n",
    "        # function will not work\n",
    "        current_items = filter_outfits(first_score, item_dict, beam_length)\n",
    "    else:\n",
    "        count = 0\n",
    "        current_items = []\n",
    "        while count < beam_length: \n",
    "            outfit = heapq.heappop(first_score)\n",
    "            items = outfit[1]\n",
    "            current_items.append(items)\n",
    "            count += 1\n",
    "        \n",
    "    return current_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@anvil.server.callable\n",
    "def get_next_best_product(files, req_categories=None):\n",
    "    size = 224, 224\n",
    "    embed_dim = 1280\n",
    "    max_len = 8\n",
    "    outfit_embeddings = []\n",
    "    outfit_categories = []\n",
    "    for file in files:\n",
    "        with anvil.media.TempFile(file) as filename:\n",
    "            image = tf.keras.preprocessing.image.load_img(filename)\n",
    "            image_arr = tf.keras.preprocessing.image.img_to_array(image)\n",
    "            w, h, c = image_arr.shape\n",
    "            aspect = h*1.0/w\n",
    "            if aspect > 1.5:\n",
    "                image_arr = tf.image.resize_with_pad(image_arr, target_height=224, target_width=224).numpy()\n",
    "            else:\n",
    "                image_arr = tf.image.resize(image_arr, size).numpy()\n",
    "            image_arr /= 255.0\n",
    "            image_embed = tf.squeeze(eff2_model(tf.expand_dims(image_arr, 0)))\n",
    "            image_cat_id, image_cat = get_product_category_2(image_arr)\n",
    "            outfit_embeddings.append(image_embed)\n",
    "            outfit_categories.append(image_cat)\n",
    "\n",
    "    print(outfit_categories)\n",
    "    if type(req_categories) is str:\n",
    "        req_categories = [req_categories]\n",
    "    outfits = create_outfit_2(query=outfit_embeddings,\n",
    "                              category=outfit_categories,\n",
    "                              required=req_categories,\n",
    "                              model=compatibility_model,\n",
    "                              image_dict=image_embedding_dict.copy(), \n",
    "                              item_dict=pv_items.copy(), \n",
    "                              beam_length=3)\n",
    "    print(outfits)\n",
    "    outfit_images = [o[-1] for o in outfits]\n",
    "    res = []\n",
    "    for item in outfit_images:\n",
    "        filepath = os.path.join(image_dir, item + \".jpg\")\n",
    "        res.append(anvil.media.from_file(filepath, 'image/jpeg'))\n",
    "    return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_files = [image for image in glob.glob(f\"{image_dir}/*.jpg\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_similar_images(similar_items):\n",
    "    nrows, ncols = 1, len(similar_items)\n",
    "    figure, axis = plt.subplots(nrows, ncols, figsize=(20,5))\n",
    "    items = [tup[1] for tup in similar_items]\n",
    "    for ii in range(ncols):\n",
    "        item = items[ii]\n",
    "        img = plt.imread(os.path.join(image_dir, item + \".jpg\"))\n",
    "        imgplot = axis[ii].imshow(img)\n",
    "        axis[ii].axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat2items.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Counter(all_scats).most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_order = ['shoes', 'jewellery', 'bags', 'tops', 'bottoms', \n",
    "                  'all-body', 'outerwear', 'sunglasses', 'accessories', 'hats', 'scarves']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the distribution over categories from outfits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_json = \"train.json\"\n",
    "\n",
    "with open(os.path.join(train_dir, train_json), 'r') as fr:\n",
    "    pv_train_pos = json.load(fr)\n",
    "\n",
    "outfit_vectors = []\n",
    "for outfit in tqdm(pv_train_pos):\n",
    "    one_hot = [0] * len(category_order)\n",
    "    items = [x['item_id'] for x in outfit['items']]\n",
    "    item_cats = [pv_items[item][\"semantic_category\"] for item in items]\n",
    "    for ii, c in enumerate(category_order):\n",
    "        if c in item_cats:\n",
    "            one_hot[ii] += item_cats.count(c)\n",
    "    outfit_vectors.append(one_hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "ovec = np.array(outfit_vectors)\n",
    "ovec.mean(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be seen that the outfit category distribution is more or less similar to the original category_order, i.e., shoes are the most prevalent one, followed by jewellery etc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query = \"/recsys_data/RecSys/fashion/129978068.jpg\"\n",
    "# image = tf.keras.preprocessing.image.load_img(query)\n",
    "# image_arr = tf.keras.preprocessing.image.img_to_array(image)\n",
    "# image_arr = tf.image.resize(image_arr, (224, 224)).numpy()\n",
    "# # image_arr = tf.image.resize_with_pad(image_arr, target_height=224, target_width=224).numpy()\n",
    "# image_arr /= 255.0\n",
    "# image_embed = tf.squeeze(eff2_model(tf.expand_dims(image_arr, 0)))\n",
    "# image_cat_id, image_cat = get_product_category_2(image_arr)\n",
    "# print(image_cat_id, image_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# outfits = create_outfit(query=image_embed,\n",
    "#                         category=image_cat,\n",
    "#                         model=model,\n",
    "#                         image_dict=image_embedding_dict.copy(), \n",
    "#                         item_dict=pv_items.copy(), max_item=3, beam_length=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fashion Image Detection - Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path = \"/recsys_data/RecSys/PyTorch-YOLOv3/data/street_samples/download_3.jpg\"\n",
    "\n",
    "temp_dir = tempfile.TemporaryDirectory()\n",
    "output_path = tempfile.TemporaryDirectory()\n",
    "shutil.copyfile(img_path, temp_dir.name+\"/test.jpg\")\n",
    "\n",
    "dataloader = _create_data_loader(temp_dir.name, batch_size=1, img_size=416, n_cpu=6)\n",
    "img_detections, imgs = detect(\n",
    "    model,\n",
    "    dataloader,\n",
    "    output_path.name,\n",
    "    conf_thres=0.5,\n",
    "    nms_thres=0.4)\n",
    "\n",
    "opaths, details = _draw_and_save_output_images(\n",
    "    img_detections, imgs, img_size, output_path.name, classes)\n",
    "\n",
    "print(temp_dir.name, output_path.name, opaths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im = np.array(Image.open(img_path))\n",
    "for x1, y1, x2, y2, conf, cls_pred in details[0]:\n",
    "    pred_class = classes[int(cls_pred)]\n",
    "    if pred_class in (\"outer\", \"dress\", \"pants\", \"top\", \"shorts\", \"skirt\"):\n",
    "        bbox = [int(x1), int(y1), int(x2), int(y2)]\n",
    "        plt.imshow(im[int(y1):int(y2), int(x1):int(x2), :])\n",
    "        plt.show()\n",
    "        A = im[int(y1):int(y2), int(x1):int(x2), :]\n",
    "        similar_prods = get_similar_products_from_array(image_arr=A, categories=[modanet2polyvore[pred_class]])\n",
    "        print(pred_class)\n",
    "        plot_similar_images(similar_prods)\n",
    "        # im2 = Image.fromarray(A)\n",
    "        # im2.save(\"your_file.jpeg\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image.open(opaths[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im = np.array(Image.open(img_path))\n",
    "print(im.shape)\n",
    "# plt.imshow(im[274:298, 82:103, :])\n",
    "plt.imshow(im[81:176, 41:128, :])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image.open(img_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use temp_dir, and when done:\n",
    "temp_dir.cleanup()\n",
    "output_path.cleanup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
