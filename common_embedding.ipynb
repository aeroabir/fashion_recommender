{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Joint Modeling of Zalando & Polyvore \n",
    "\n",
    " - Create an image embedding that understands both the types of data\n",
    " - Use the product catgeory information that is assumed to be available as the target\n",
    " - use this embedding for subsequent outfit generation/compatibility learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n",
      "Pytorch: 1.10.0+cu102\n",
      "NP: 1.19.5\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import json\n",
    "from collections import Counter\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torchvision.models import resnet50\n",
    "\n",
    "# import tensorflow as tf\n",
    "# from tensorflow import keras\n",
    "# from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from numpy.random import default_rng\n",
    "\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "from prettytable import PrettyTable\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, \"/recsys_data/RecSys/fashion/automl/efficientnetv2\")\n",
    "import effnetv2_model\n",
    "\n",
    "%pylab inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"Pytorch:\", torch.__version__)\n",
    "print(\"NP:\", np.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "zalando_dir = \"/recsys_data/RecSys/Zalando_Outfit/female/Outfit_Data\"\n",
    "zalando_image_dir = \"/recsys_data/RecSys/Zalando_Outfit/resized_packshot_images_female\"\n",
    "\n",
    "pv_base_dir = \"/recsys_data/RecSys/fashion/polyvore-dataset/polyvore_outfits\"\n",
    "pv_image_dir = os.path.join(pv_base_dir, \"images\")\n",
    "pv_item_file = \"polyvore_item_metadata.json\"\n",
    "\n",
    "pv2zalando = {'accessories': 'accessory',\n",
    "              'all-body': 'all-body',\n",
    "              'bags': 'accessory',\n",
    "              'bottoms': 'bottomwear',\n",
    "              'hats': 'accessory',\n",
    "              'jewellery': 'jewellery',\n",
    "              'outerwear': 'outerwear',\n",
    "              'scarves': 'accessory',\n",
    "              'shoes': 'footwear',\n",
    "              'sunglasses': 'accessory',\n",
    "              'tops': 'topwear'}\n",
    "\n",
    "zalando2pv = {'all-body': 'all-body', \n",
    "              'footwear': 'shoes', \n",
    "              'accessory': 'accessories', \n",
    "              'outerwear': 'outerwear', \n",
    "              'jewellery': 'jewellery', \n",
    "              'topwear': 'tops', \n",
    "              'bottomwear': 'bottoms', \n",
    "#               'bodywear_nightwear_innerwear': None, \n",
    "#               'beachwear_swimwear': None\n",
    "             }\n",
    "\n",
    "with open(os.path.join(pv_base_dir, pv_item_file), 'r') as fr:\n",
    "    pv_items = json.load(fr)\n",
    "\n",
    "pv_item2cat = dict()\n",
    "for item_id in pv_items:\n",
    "    pv_item2cat[item_id] = pv_items[item_id][\"semantic_category\"]\n",
    "\n",
    "zalando_item2cat = dict()\n",
    "with open(os.path.join(zalando_dir, \"item2cat.txt\"), 'r') as fr:\n",
    "    for line in fr:\n",
    "        item, cat = line.strip().split()\n",
    "        zalando_item2cat[item] = cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read 251008 polyvore images\n",
      "Read 35576 Zalando images\n"
     ]
    }
   ],
   "source": [
    "all_paths = dict()\n",
    "count = 0\n",
    "for image_path in glob.glob(pv_image_dir + \"/*.jpg\"):\n",
    "    image = image_path.split(\"/\")[-1]\n",
    "    item_id = image.split(\".\")[0]\n",
    "    if item_id in pv_item2cat:\n",
    "        all_paths[image_path] = pv_item2cat[item_id]\n",
    "        count += 1\n",
    "print(f\"Read {count} polyvore images\")\n",
    "\n",
    "count = 0\n",
    "for image_path in glob.glob(zalando_image_dir + \"/*.jpg\"):\n",
    "    image = image_path.split(\"/\")[-1]\n",
    "    if image in zalando_item2cat:\n",
    "        zcat = zalando_item2cat[image]\n",
    "        if zcat in zalando2pv:\n",
    "            all_paths[image_path] = zalando2pv[zcat]\n",
    "        count += 1\n",
    "print(f\"Read {count} Zalando images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "196279 35576 51008\n"
     ]
    }
   ],
   "source": [
    "data = list(all_paths.items())\n",
    "rng = default_rng(seed=100)\n",
    "num_valid = 35576\n",
    "num_test = 51008\n",
    "\n",
    "test_indices = rng.choice(len(data), size=num_test, replace=False)\n",
    "test_paths = [data[ii] for ii in test_indices]\n",
    "rem_paths = [data[ii] for ii in range(len(data)) if ii not in test_indices]\n",
    "\n",
    "val_indices = rng.choice(len(rem_paths), size=num_valid, replace=False)\n",
    "val_paths = [rem_paths[ii] for ii in val_indices]\n",
    "train_paths = [rem_paths[ii] for ii in range(len(rem_paths)) if ii not in val_indices]\n",
    "print(len(train_paths), len(val_paths), len(test_paths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/recsys_data/RecSys/fashion/polyvore-dataset/polyvore_outfits/images/200162527.jpg outerwear\n",
      "/recsys_data/RecSys/fashion/polyvore-dataset/polyvore_outfits/images/129089366.jpg bags\n",
      "/recsys_data/RecSys/fashion/polyvore-dataset/polyvore_outfits/images/191335829.jpg outerwear\n",
      "/recsys_data/RecSys/fashion/polyvore-dataset/polyvore_outfits/images/128377889.jpg hats\n",
      "/recsys_data/RecSys/fashion/polyvore-dataset/polyvore_outfits/images/212676450.jpg bags\n"
     ]
    }
   ],
   "source": [
    "for item_id, cat in train_paths[0:5]:\n",
    "    print(item_id, cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total 1000 images with 11 classes\n",
      "Total 1000 images with 11 classes\n",
      "Total 1000 images with 11 classes\n"
     ]
    }
   ],
   "source": [
    "from utils_torch import ImageDataGen\n",
    "label_dict = {'accessories': 0, \n",
    "              'outerwear': 1, \n",
    "              'bags': 2, \n",
    "              'shoes': 3, \n",
    "              'all-body': 4, \n",
    "              'hats': 5, \n",
    "              'scarves': 6, \n",
    "              'jewellery': 7, \n",
    "              'sunglasses': 8, \n",
    "              'bottoms': 9, \n",
    "              'tops': 10}\n",
    "\n",
    "train_set = ImageDataGen(train_paths,\n",
    "                         input_size=(3, 224, 224),\n",
    "                         label_dict=label_dict,\n",
    "                         # max_example=1000,\n",
    "                        )\n",
    "\n",
    "valid_set = ImageDataGen(val_paths,\n",
    "                         input_size=(3, 224, 224),\n",
    "                         label_dict=label_dict,\n",
    "                        )\n",
    "\n",
    "test_set = ImageDataGen(test_paths,\n",
    "                        input_size=(3, 224, 224),\n",
    "                        label_dict=label_dict,\n",
    "                        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 224, 224]) torch.Size([])\n",
      "torch.Size([3, 224, 224]) torch.Size([])\n",
      "torch.Size([3, 224, 224]) torch.Size([])\n",
      "torch.Size([3, 224, 224]) torch.Size([])\n"
     ]
    }
   ],
   "source": [
    "for ii in range(4):\n",
    "    inps, targs = train_set[ii]\n",
    "    print(inps.shape, targs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageClassifier(torch.nn.Module):\n",
    "    def __init__(self, d_model, num_classes):\n",
    "        super(ImageClassifier, self).__init__()\n",
    "        # Get a resnet50 backbone\n",
    "        m = resnet50()\n",
    "        # Extract 4 main layers (note: MaskRCNN needs this particular name\n",
    "        # mapping for return nodes)\n",
    "        # self.body = create_feature_extractor(\n",
    "        #    m, return_nodes={f'layer{k}': str(v)\n",
    "        #                     for v, k in enumerate([1, 2, 3, 4])})\n",
    "        self.body = m\n",
    "        self.image_projector = torch.nn.Sequential(\n",
    "            torch.nn.Linear(1000, d_model),\n",
    "            torch.nn.Tanh())\n",
    "        \n",
    "        # for NLL loss\n",
    "#         self.final = torch.nn.Sequential(\n",
    "#             torch.nn.Linear(d_model, num_classes),\n",
    "#             torch.nn.LogSoftmax(dim=1))\n",
    "\n",
    "        # for categorical-crossentropy loss\n",
    "        self.final = torch.nn.Sequential(\n",
    "            torch.nn.Linear(d_model, num_classes),\n",
    "            torch.nn.ReLU())\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.body(x)\n",
    "        y = self.image_projector(x)\n",
    "        y = self.final(y)\n",
    "        return y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Trainable Params: 25816115\n"
     ]
    }
   ],
   "source": [
    "model = ImageClassifier(256, 11)\n",
    "model.to('cuda')\n",
    "total_params = 0\n",
    "for name, parameter in model.named_parameters():\n",
    "    if not parameter.requires_grad:\n",
    "        continue\n",
    "    param = parameter.numel()\n",
    "    total_params += param\n",
    "print(f\"Total Trainable Params: {total_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Epoch   0 | time: 37.13s | train-loss 2.3265 | val-loss 6.9083 | val-ACC 0.1680 ( 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Epoch   1 | time: 37.80s | train-loss 2.2627 | val-loss 6.8811 | val-ACC 0.2020 ( 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Epoch   2 | time: 37.38s | train-loss 2.1750 | val-loss 6.8823 | val-ACC 0.1490 ( 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Epoch   3 | time: 37.56s | train-loss 2.1215 | val-loss 6.8622 | val-ACC 0.1760 ( 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Epoch   4 | time: 38.00s | train-loss 2.0630 | val-loss 6.9173 | val-ACC 0.1730 ( 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Epoch   5 | time: 37.48s | train-loss 2.0757 | val-loss 6.8841 | val-ACC 0.1580 ( 4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Epoch   6 | time: 37.21s | train-loss 1.9532 | val-loss 6.9166 | val-ACC 0.1530 ( 5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Epoch   7 | time: 37.52s | train-loss 1.9227 | val-loss 6.9580 | val-ACC 0.1570 ( 6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Epoch   8 | time: 37.11s | train-loss 1.8768 | val-loss 6.9649 | val-ACC 0.1560 ( 7)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Epoch   9 | time: 37.75s | train-loss 1.8178 | val-loss 6.9916 | val-ACC 0.1500 ( 8)\n",
      "Best valid Accuracy: 0.2020\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test results: {'acc': 0.123, 'loss': 6.93646}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "from utils_torch import train\n",
    "\n",
    "batch_size = 64\n",
    "model_path = \"/recsys_data/RecSys/fashion/finetuned_resnet50.pt\"\n",
    "val_perf, test_perf = train(model, \n",
    "                          train_set, \n",
    "                          valid_set, \n",
    "                          device='cuda', \n",
    "                          epochs=10, \n",
    "                          batch_size=batch_size,\n",
    "                          learning_rate=1e-04,\n",
    "                          loss_name=\"xent\",\n",
    "                          observe=\"acc\", \n",
    "                          test_set=test_set,\n",
    "                          model_path=model_path,\n",
    "                         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.load_state_dict(torch.load(\"/recsys_data/RecSys/fashion/finetuned_resnet.pt\"))\n",
    "# model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = ImageClassifier(256, 11).eval()\n",
    "# y = model(torch.unsqueeze(inps, 0))\n",
    "# y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list(all_paths.keys())[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list(all_paths.keys())[-5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# im = Image.open('/recsys_data/RecSys/fashion/polyvore-dataset/polyvore_outfits/images/114082981.jpg').convert('RGB')\n",
    "# np.array(im).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# im = Image.open('/recsys_data/RecSys/Zalando_Outfit/resized_packshot_images_female/M0Q21A0BP-G11@10.jpg').convert('RGB')\n",
    "# np.array(im).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
