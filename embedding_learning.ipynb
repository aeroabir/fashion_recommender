{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learn Graph Sage Embedding\n",
    "\n",
    "GraphSage embeddings work beautifully on the training dataset but not on the validation data and there is no reason it'll work on the test dataset. However, for each item we have all the features (text, image, category etc.) and we can use them to learn the GraphSage embeddings. The training dataset for this model will come from the items present in the original training dataset and the model will be evaluated on the items present *only* in the validation and test dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from collections import Counter\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import sys\n",
    "\n",
    "%pylab inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load all the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = \"/recsys_data/RecSys/fashion/polyvore-dataset/polyvore_outfits\"\n",
    "data_type = \"nondisjoint\" # \"nondisjoint\", \"disjoint\"\n",
    "train_dir = os.path.join(base_dir, data_type)\n",
    "image_dir = os.path.join(base_dir, \"images\")\n",
    "embed_dir = \"/recsys_data/RecSys/fashion/polyvore-dataset/precomputed\"\n",
    "\n",
    "train_json = \"train.json\"\n",
    "valid_json = \"valid.json\"\n",
    "test_json = \"test.json\"\n",
    "\n",
    "train_file = \"compatibility_train.txt\"\n",
    "valid_file = \"compatibility_valid.txt\"\n",
    "test_file = \"compatibility_test.txt\"\n",
    "item_file = \"polyvore_item_metadata.json\"\n",
    "outfit_file = \"polyvore_outfit_titles.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(train_dir, train_json), 'r') as fr:\n",
    "    train_pos = json.load(fr)\n",
    "    \n",
    "with open(os.path.join(train_dir, valid_json), 'r') as fr:\n",
    "    valid_pos = json.load(fr)\n",
    "    \n",
    "with open(os.path.join(train_dir, test_json), 'r') as fr:\n",
    "    test_pos = json.load(fr)\n",
    "    \n",
    "with open(os.path.join(base_dir, item_file), 'r') as fr:\n",
    "    pv_items = json.load(fr)\n",
    "    \n",
    "with open(os.path.join(base_dir, outfit_file), 'r') as fr:\n",
    "    pv_outfits = json.load(fr)\n",
    "\n",
    "with open(os.path.join(train_dir, train_file), 'r') as fr:\n",
    "    train_X, train_y = [], []\n",
    "    for line in fr:\n",
    "        elems = line.strip().split()\n",
    "        train_y.append(elems[0])\n",
    "        train_X.append(elems[1:])\n",
    "\n",
    "with open(os.path.join(train_dir, valid_file), 'r') as fr:\n",
    "    valid_X, valid_y = [], []\n",
    "    for line in fr:\n",
    "        elems = line.strip().split()\n",
    "        valid_y.append(elems[0])\n",
    "        valid_X.append(elems[1:])\n",
    "\n",
    "with open(os.path.join(train_dir, test_file), 'r') as fr:\n",
    "    test_X, test_y = [], []\n",
    "    for line in fr:\n",
    "        elems = line.strip().split()\n",
    "        test_y.append(elems[0])\n",
    "        test_X.append(elems[1:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total 204679 items in the train data\n",
      "Total 25132 items in the valid data\n",
      "9356 common items between train and validation set\n",
      "Total 47854 items in the test data\n",
      "16655 common items between train and test set\n"
     ]
    }
   ],
   "source": [
    "train_set = set()\n",
    "for outfit in train_pos:\n",
    "    items = [x['item_id'] for x in outfit['items']]\n",
    "    train_set |= set(items)\n",
    "print(f\"Total {len(train_set)} items in the train data\")\n",
    "\n",
    "valid_set = set()\n",
    "for outfit in valid_pos:\n",
    "    items = [x['item_id'] for x in outfit['items']]\n",
    "    valid_set |= set(items)\n",
    "print(f\"Total {len(valid_set)} items in the valid data\")\n",
    "print(f\"{len(valid_set.intersection(train_set))} common items between train and validation set\")\n",
    "\n",
    "test_set = set()\n",
    "for outfit in test_pos:\n",
    "    items = [x['item_id'] for x in outfit['items']]\n",
    "    test_set |= set(items)\n",
    "print(f\"Total {len(test_set)} items in the test data\")\n",
    "print(f\"{len(test_set.intersection(train_set))} common items between train and test set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "153"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_item_categories = set([pv_items[item]['category_id'] for item in pv_items])\n",
    "len(all_item_categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_renum_dict = {}\n",
    "for ii, k in enumerate(all_item_categories):\n",
    "    label_renum_dict[k] = ii"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load all the embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(embed_dir, \"effnet2_polyvore.pkl\"), \"rb\") as fr:\n",
    "    image_embedding = pickle.load(fr)\n",
    "    \n",
    "with open(os.path.join(embed_dir, \"bert_polyvore.pkl\"), \"rb\") as fr:\n",
    "    text_embedding = pickle.load(fr)\n",
    "\n",
    "with open(os.path.join(embed_dir, \"graphsage_dict_polyvore_nondisjoint.pkl\"), \"rb\") as fr:\n",
    "    graphsage_embedding = pickle.load(fr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 204679/204679 [00:02<00:00, 94512.53it/s]\n"
     ]
    }
   ],
   "source": [
    "train_X1, train_X2 = [], []\n",
    "train_Y = []\n",
    "train_item_list = []\n",
    "count = 0\n",
    "for item in tqdm(train_set):\n",
    "    train_item_list.append(item)\n",
    "    train_X1.append(image_embedding[item].numpy())\n",
    "    train_X2.append(label_renum_dict[pv_items[item]['category_id']])\n",
    "    train_Y.append(graphsage_embedding[item])\n",
    "    count += 1\n",
    "train_X1 = np.array(train_X1)\n",
    "train_X2 = np.array(train_X2)\n",
    "train_Y = np.array(train_Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build an Embedding Mapping Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, 1, 100)       15300       input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "input_1 (InputLayer)            [(None, 1280)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Squeeze (TensorFlow [(None, 100)]        0           embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 1380)         0           input_1[0][0]                    \n",
      "                                                                 tf_op_layer_Squeeze[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 256)          353536      concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 256)          65792       dense[0][0]                      \n",
      "==================================================================================================\n",
      "Total params: 434,628\n",
      "Trainable params: 434,628\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "image_dim = 1280\n",
    "hidden_dim = 256\n",
    "out_dim = 256 # 50\n",
    "\n",
    "in1 = tf.keras.layers.Input(shape=(image_dim))\n",
    "in2 = tf.keras.layers.Input(shape=(1))\n",
    "x2 = tf.keras.layers.Embedding(153, 100)(in2)\n",
    "x2 = tf.squeeze(x2, -2)\n",
    "x3 = tf.keras.layers.concatenate([in1, x2], axis=-1)\n",
    "out = tf.keras.layers.Dense(hidden_dim, activation=\"relu\")(x3)\n",
    "out = tf.keras.layers.Dense(out_dim, activation=\"linear\")(out)\n",
    "model = tf.keras.models.Model(inputs=[in1, in2], outputs=out)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "800/800 [==============================] - 4s 4ms/step - loss: 0.0066 - mse: 0.0066 - mae: 0.0506\n",
      "Epoch 2/50\n",
      "800/800 [==============================] - 4s 5ms/step - loss: 0.0039 - mse: 0.0039 - mae: 0.0367\n",
      "Epoch 3/50\n",
      "800/800 [==============================] - 4s 5ms/step - loss: 0.0039 - mse: 0.0039 - mae: 0.0360\n",
      "Epoch 4/50\n",
      "800/800 [==============================] - 4s 5ms/step - loss: 0.0038 - mse: 0.0038 - mae: 0.0358\n",
      "Epoch 5/50\n",
      "800/800 [==============================] - 4s 5ms/step - loss: 0.0038 - mse: 0.0038 - mae: 0.0357\n",
      "Epoch 6/50\n",
      "800/800 [==============================] - 4s 5ms/step - loss: 0.0038 - mse: 0.0038 - mae: 0.0356\n",
      "Epoch 7/50\n",
      "800/800 [==============================] - 4s 5ms/step - loss: 0.0038 - mse: 0.0038 - mae: 0.0357\n",
      "Epoch 8/50\n",
      "800/800 [==============================] - 4s 4ms/step - loss: 0.0038 - mse: 0.0038 - mae: 0.0353\n",
      "Epoch 9/50\n",
      "800/800 [==============================] - 4s 5ms/step - loss: 0.0036 - mse: 0.0036 - mae: 0.0346\n",
      "Epoch 10/50\n",
      "800/800 [==============================] - 4s 5ms/step - loss: 0.0036 - mse: 0.0036 - mae: 0.0345\n",
      "Epoch 11/50\n",
      "800/800 [==============================] - 4s 4ms/step - loss: 0.0036 - mse: 0.0036 - mae: 0.0343\n",
      "Epoch 12/50\n",
      "800/800 [==============================] - 4s 5ms/step - loss: 0.0035 - mse: 0.0035 - mae: 0.0341\n",
      "Epoch 13/50\n",
      "800/800 [==============================] - 4s 4ms/step - loss: 0.0035 - mse: 0.0035 - mae: 0.0337\n",
      "Epoch 14/50\n",
      "800/800 [==============================] - 4s 4ms/step - loss: 0.0034 - mse: 0.0034 - mae: 0.0332\n",
      "Epoch 15/50\n",
      "800/800 [==============================] - 4s 5ms/step - loss: 0.0033 - mse: 0.0033 - mae: 0.0327\n",
      "Epoch 16/50\n",
      "800/800 [==============================] - 4s 5ms/step - loss: 0.0032 - mse: 0.0032 - mae: 0.0324\n",
      "Epoch 17/50\n",
      "800/800 [==============================] - 4s 5ms/step - loss: 0.0032 - mse: 0.0032 - mae: 0.0322\n",
      "Epoch 18/50\n",
      "800/800 [==============================] - 4s 5ms/step - loss: 0.0032 - mse: 0.0032 - mae: 0.0321\n",
      "Epoch 19/50\n",
      "800/800 [==============================] - 4s 5ms/step - loss: 0.0031 - mse: 0.0031 - mae: 0.0320\n",
      "Epoch 20/50\n",
      "800/800 [==============================] - 4s 5ms/step - loss: 0.0031 - mse: 0.0031 - mae: 0.0319\n",
      "Epoch 21/50\n",
      "800/800 [==============================] - 4s 5ms/step - loss: 0.0031 - mse: 0.0031 - mae: 0.0318\n",
      "Epoch 22/50\n",
      "800/800 [==============================] - 4s 4ms/step - loss: 0.0031 - mse: 0.0031 - mae: 0.0318\n",
      "Epoch 23/50\n",
      "800/800 [==============================] - 4s 4ms/step - loss: 0.0031 - mse: 0.0031 - mae: 0.0317\n",
      "Epoch 24/50\n",
      "800/800 [==============================] - 4s 5ms/step - loss: 0.0031 - mse: 0.0031 - mae: 0.0316\n",
      "Epoch 25/50\n",
      "800/800 [==============================] - 4s 5ms/step - loss: 0.0031 - mse: 0.0031 - mae: 0.0316\n",
      "Epoch 26/50\n",
      "800/800 [==============================] - 4s 4ms/step - loss: 0.0030 - mse: 0.0030 - mae: 0.0315\n",
      "Epoch 27/50\n",
      "800/800 [==============================] - 4s 5ms/step - loss: 0.0030 - mse: 0.0030 - mae: 0.0315\n",
      "Epoch 28/50\n",
      "800/800 [==============================] - 4s 4ms/step - loss: 0.0030 - mse: 0.0030 - mae: 0.0315\n",
      "Epoch 29/50\n",
      "800/800 [==============================] - 4s 4ms/step - loss: 0.0030 - mse: 0.0030 - mae: 0.0315\n",
      "Epoch 30/50\n",
      "800/800 [==============================] - 4s 5ms/step - loss: 0.0030 - mse: 0.0030 - mae: 0.0314\n",
      "Epoch 31/50\n",
      "800/800 [==============================] - 4s 4ms/step - loss: 0.0030 - mse: 0.0030 - mae: 0.0314\n",
      "Epoch 32/50\n",
      "800/800 [==============================] - 4s 4ms/step - loss: 0.0030 - mse: 0.0030 - mae: 0.0314\n",
      "Epoch 33/50\n",
      "800/800 [==============================] - 3s 4ms/step - loss: 0.0030 - mse: 0.0030 - mae: 0.0313\n",
      "Epoch 34/50\n",
      "800/800 [==============================] - 4s 4ms/step - loss: 0.0030 - mse: 0.0030 - mae: 0.0313\n",
      "Epoch 35/50\n",
      "800/800 [==============================] - 4s 5ms/step - loss: 0.0030 - mse: 0.0030 - mae: 0.0312\n",
      "Epoch 36/50\n",
      "800/800 [==============================] - 4s 5ms/step - loss: 0.0030 - mse: 0.0030 - mae: 0.0312\n",
      "Epoch 37/50\n",
      "800/800 [==============================] - 4s 4ms/step - loss: 0.0030 - mse: 0.0030 - mae: 0.0312\n",
      "Epoch 38/50\n",
      "800/800 [==============================] - 4s 4ms/step - loss: 0.0030 - mse: 0.0030 - mae: 0.0311\n",
      "Epoch 39/50\n",
      "800/800 [==============================] - 4s 5ms/step - loss: 0.0030 - mse: 0.0030 - mae: 0.0311\n",
      "Epoch 40/50\n",
      "800/800 [==============================] - 4s 4ms/step - loss: 0.0030 - mse: 0.0030 - mae: 0.0310\n",
      "Epoch 41/50\n",
      "800/800 [==============================] - 4s 5ms/step - loss: 0.0030 - mse: 0.0030 - mae: 0.0310\n",
      "Epoch 42/50\n",
      "800/800 [==============================] - 4s 5ms/step - loss: 0.0030 - mse: 0.0030 - mae: 0.0309\n",
      "Epoch 43/50\n",
      "800/800 [==============================] - 4s 4ms/step - loss: 0.0029 - mse: 0.0029 - mae: 0.0309\n",
      "Epoch 44/50\n",
      "800/800 [==============================] - 4s 5ms/step - loss: 0.0029 - mse: 0.0029 - mae: 0.0308\n",
      "Epoch 45/50\n",
      "800/800 [==============================] - 4s 5ms/step - loss: 0.0029 - mse: 0.0029 - mae: 0.0308\n",
      "Epoch 46/50\n",
      "800/800 [==============================] - 4s 4ms/step - loss: 0.0029 - mse: 0.0029 - mae: 0.0308\n",
      "Epoch 47/50\n",
      "800/800 [==============================] - 4s 4ms/step - loss: 0.0029 - mse: 0.0029 - mae: 0.0307\n",
      "Epoch 48/50\n",
      "800/800 [==============================] - 4s 5ms/step - loss: 0.0029 - mse: 0.0029 - mae: 0.0307\n",
      "Epoch 49/50\n",
      "800/800 [==============================] - 4s 4ms/step - loss: 0.0029 - mse: 0.0029 - mae: 0.0307\n",
      "Epoch 50/50\n",
      "800/800 [==============================] - 4s 5ms/step - loss: 0.0029 - mse: 0.0029 - mae: 0.0306\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "182.926189661026"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learning_rate = 1.0e-04\n",
    "batch_size = 256\n",
    "epochs = 50\n",
    "patience = 5\n",
    "\n",
    "opt = keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "model.compile(loss='mse', optimizer=opt, metrics=[\"mse\", \"mae\"])  # 'adam'\n",
    "callback = EarlyStopping(\n",
    "                        monitor=\"val_accuracy\",\n",
    "                        min_delta=0,\n",
    "                        patience=patience,\n",
    "                        verbose=0,\n",
    "                        mode=\"auto\",\n",
    "                        baseline=None,\n",
    "                        restore_best_weights=True,\n",
    "                    )\n",
    "tic = time.time()\n",
    "history = model.fit([train_X1, train_X2], train_Y, \n",
    "                    epochs=epochs, \n",
    "                    batch_size=batch_size,\n",
    "                    steps_per_epoch=math.ceil(train_X1.shape[0]/batch_size),\n",
    "#                     validation_data=(val_X, val_y),\n",
    "#                     callbacks=[callback, model_checkpoint_callback],\n",
    "                    verbose=1)\n",
    "time.time() - tic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_X1, valid_X2 = [], []\n",
    "valid_item_list = []\n",
    "for item in valid_set:\n",
    "    valid_item_list.append(item)\n",
    "    valid_X1.append(image_embedding[item].numpy())\n",
    "    valid_X2.append(label_renum_dict[pv_items[item]['category_id']])\n",
    "valid_X1 = np.array(valid_X1)\n",
    "valid_X2 = np.array(valid_X2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_Y = model([valid_X1, valid_X2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([25132, 256])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_Y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write the Model Prediction\n",
    "\n",
    " - Keep the embedding of the training items same as before\n",
    " - Update only the new items present in the validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_graphsage_dict = {}\n",
    "for item in train_set:\n",
    "    new_graphsage_dict[item] = graphsage_embedding[item]\n",
    "\n",
    "for item in valid_set:\n",
    "    if item not in train_set:\n",
    "        jj = valid_item_list.index(item)\n",
    "        new_graphsage_dict[item] = valid_Y[jj]\n",
    "        \n",
    "with open(f\"graphsage_dict2_polyvore_{data_type}.pkl\", \"wb\") as output_file:\n",
    "    pickle.dump(new_graphsage_dict, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
