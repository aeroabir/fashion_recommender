{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Few-Shot Learning \n",
    "\n",
    "Apply Few-shot learning paradigm for outfit compatibility learning from few examples. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import json\n",
    "import math\n",
    "import numpy as np\n",
    "import pickle\n",
    "import random\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "from prettytable import PrettyTable\n",
    "\n",
    "import torch\n",
    "import torch.utils.data as torch_data\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score\n",
    "\n",
    "from tensorflow.random import set_seed\n",
    "from numpy.random import seed\n",
    "from numpy.random import default_rng\n",
    "\n",
    "import shap\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = \"/recsys_data/RecSys/fashion/polyvore-dataset/polyvore_outfits\"\n",
    "data_type = \"nondisjoint\" # \"nondisjoint\", \"disjoint\"\n",
    "train_dir = os.path.join(base_dir, data_type)\n",
    "image_dir = os.path.join(base_dir, \"images\")\n",
    "embed_dir = \"/recsys_data/RecSys/fashion/polyvore-dataset/precomputed\"\n",
    "train_json = \"train.json\"\n",
    "valid_json = \"valid.json\"\n",
    "test_json = \"test.json\"\n",
    "\n",
    "train_file = \"compatibility_train.txt\"\n",
    "valid_file = \"compatibility_valid.txt\"\n",
    "test_file = \"compatibility_test.txt\"\n",
    "item_file = \"polyvore_item_metadata.json\"\n",
    "outfit_file = \"polyvore_outfit_titles.json\"\n",
    "\n",
    "model_type = \"rnn\" #\"set-transformer\"\n",
    "include_text = True\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read all the required files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total 53306, 5000, 10000 outfits in train, validation and test split, respectively\n"
     ]
    }
   ],
   "source": [
    "with open(os.path.join(train_dir, train_json), 'r') as fr:\n",
    "    train_pos = json.load(fr)\n",
    "    \n",
    "with open(os.path.join(train_dir, valid_json), 'r') as fr:\n",
    "    valid_pos = json.load(fr)\n",
    "    \n",
    "with open(os.path.join(train_dir, test_json), 'r') as fr:\n",
    "    test_pos = json.load(fr)\n",
    "    \n",
    "with open(os.path.join(base_dir, item_file), 'r') as fr:\n",
    "    pv_items = json.load(fr)\n",
    "    \n",
    "with open(os.path.join(base_dir, outfit_file), 'r') as fr:\n",
    "    pv_outfits = json.load(fr)\n",
    "\n",
    "print(f\"Total {len(train_pos)}, {len(valid_pos)}, {len(test_pos)} outfits in train, validation and test split, respectively\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total 106612, 10000, 20000 examples in train, validation and test split, respectively\n"
     ]
    }
   ],
   "source": [
    "with open(os.path.join(train_dir, train_file), 'r') as fr:\n",
    "    train_X, train_y = [], []\n",
    "    for line in fr:\n",
    "        elems = line.strip().split()\n",
    "        train_y.append(elems[0])\n",
    "        train_X.append(elems[1:])\n",
    "\n",
    "with open(os.path.join(train_dir, valid_file), 'r') as fr:\n",
    "    valid_X, valid_y = [], []\n",
    "    for line in fr:\n",
    "        elems = line.strip().split()\n",
    "        valid_y.append(elems[0])\n",
    "        valid_X.append(elems[1:])\n",
    "\n",
    "with open(os.path.join(train_dir, test_file), 'r') as fr:\n",
    "    test_X, test_y = [], []\n",
    "    for line in fr:\n",
    "        elems = line.strip().split()\n",
    "        test_y.append(elems[0])\n",
    "        test_X.append(elems[1:])\n",
    "\n",
    "print(f\"Total {len(train_X)}, {len(valid_X)}, {len(test_X)} examples in train, validation and test split, respectively\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "284767\n",
      "311548\n",
      "365054\n"
     ]
    }
   ],
   "source": [
    "item_dict = {}\n",
    "for ii, outfit in enumerate(train_pos):\n",
    "    items = outfit['items']\n",
    "    mapped = train_X[ii]\n",
    "    item_dict.update({jj:kk['item_id'] for jj, kk in zip(mapped, items)})\n",
    "print(len(item_dict))\n",
    "\n",
    "for ii, outfit in enumerate(valid_pos):\n",
    "    items = outfit['items']\n",
    "    mapped = valid_X[ii]\n",
    "    item_dict.update({jj:kk['item_id'] for jj, kk in zip(mapped, items)})\n",
    "print(len(item_dict))\n",
    "\n",
    "for ii, outfit in enumerate(test_pos):\n",
    "    items = outfit['items']\n",
    "    mapped = test_X[ii]\n",
    "    item_dict.update({jj:kk['item_id'] for jj, kk in zip(mapped, items)})\n",
    "print(len(item_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_type = \"transformer\" # \"set-transformer\", \"rnn\"\n",
    "include_text = True\n",
    "use_graphsage = False\n",
    "batch_size = 32\n",
    "max_seq_len = 8\n",
    "d_model_rnn = 512\n",
    "image_data_type = \"embedding\"  # \"original\", \"embedding\", \"both\"\n",
    "include_item_categories = True\n",
    "image_encoder = \"resnet18\"  # \"resnet50\", \"vgg16\", \"inception\"\n",
    "\n",
    "if use_graphsage:\n",
    "    image_embedding_dim, image_embedding_file = (50, os.path.join(embed_dir, \"graphsage_dict2_polyvore.pkl\"))\n",
    "#         image_embedding_dim, image_embedding_file = (256, os.path.join(embed_dir, \"graphsage_dict2_polyvore_nondisjoint.pkl\"))\n",
    "else:\n",
    "    image_embedding_dim, image_embedding_file = (1280, os.path.join(embed_dir, \"effnet_tuned_polyvore.pkl\"))\n",
    "#         image_embedding_dim, image_embedding_file = (256, os.path.join(embed_dir, \"triplet_polyvore_image.pkl\"))\n",
    "    \n",
    "text_embedding_dim, text_embedding_file = (768, os.path.join(embed_dir, \"bert_polyvore.pkl\"))\n",
    "num_support, num_query = 100, 100\n",
    "num_episodes = 1\n",
    "num_train_epochs = 20000\n",
    "num_test_samples = 1000\n",
    "learning_rate = 1e-05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    table = PrettyTable([\"Modules\", \"Parameters\"])\n",
    "    total_params = 0\n",
    "    for name, parameter in model.named_parameters():\n",
    "        if not parameter.requires_grad: continue\n",
    "        params = parameter.numel()\n",
    "        table.add_row([name, params])\n",
    "        total_params+=params\n",
    "    print(table)\n",
    "    print(f\"Total Trainable Params: {total_params}\")\n",
    "    return total_params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "+--------------------------------------------+------------+\n",
      "|                  Modules                   | Parameters |\n",
      "+--------------------------------------------+------------+\n",
      "|          image_projector.0.weight          |   81920    |\n",
      "|           image_projector.0.bias           |     64     |\n",
      "|          text_projector.0.weight           |   49152    |\n",
      "|           text_projector.0.bias            |     64     |\n",
      "|          category_embedder.weight          |    9856    |\n",
      "| encoder.layers.0.self_attn.in_proj_weight  |   110592   |\n",
      "|  encoder.layers.0.self_attn.in_proj_bias   |    576     |\n",
      "| encoder.layers.0.self_attn.out_proj.weight |   36864    |\n",
      "|  encoder.layers.0.self_attn.out_proj.bias  |    192     |\n",
      "|      encoder.layers.0.linear1.weight       |    6144    |\n",
      "|       encoder.layers.0.linear1.bias        |     32     |\n",
      "|      encoder.layers.0.linear2.weight       |    6144    |\n",
      "|       encoder.layers.0.linear2.bias        |    192     |\n",
      "|       encoder.layers.0.norm1.weight        |    192     |\n",
      "|        encoder.layers.0.norm1.bias         |    192     |\n",
      "|       encoder.layers.0.norm2.weight        |    192     |\n",
      "|        encoder.layers.0.norm2.bias         |    192     |\n",
      "|            encoder.norm.weight             |    192     |\n",
      "|             encoder.norm.bias              |    192     |\n",
      "|          proto_embedding.0.weight          |   98304    |\n",
      "|           proto_embedding.0.bias           |     64     |\n",
      "+--------------------------------------------+------------+\n",
      "Total Trainable Params: 401312\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "401312"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from few_shot_models import SimpleProtoTypeModel, proto_loss, get_prototypes\n",
    "from torch.optim import AdamW\n",
    "from tqdm import tqdm\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# device = 'cpu'\n",
    "print(\"Device:\", device)\n",
    "\n",
    "# Model only returns the embeddings\n",
    "model = SimpleProtoTypeModel(num_layers=1,\n",
    "                             d_model=64,\n",
    "                             num_heads=4,\n",
    "                             dff=32,\n",
    "                             rate=0.0,\n",
    "                             image_data_type=image_data_type,\n",
    "                             include_text=include_text,\n",
    "                             include_item_categories=include_item_categories,\n",
    "                             num_categories=154,\n",
    "                             embedding_activation=\"linear\",\n",
    "                             encoder_activation=\"relu\",\n",
    "                             embedding_dim=64,\n",
    "                             max_seq_len=max_seq_len,\n",
    "                             num_classes=2,\n",
    "                             use_rnn=False,\n",
    "                             device=device,\n",
    "                            )\n",
    "model.to(device)\n",
    "count_parameters(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select n_support outfits for learning two prototypes (0 and 1 class) and evaluate on n_query outfits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_balanced_samples(x, y, num_samples):\n",
    "    \"\"\"\n",
    "    make sure that both the classes are present in the samples\n",
    "    \"\"\"\n",
    "    index_0 = [ii for ii in range(len(y)) if y[ii]=='0']\n",
    "    index_1 = [ii for ii in range(len(y)) if y[ii]=='1']\n",
    "    sample_0 = rng.choice(index_0, size=num_samples//2, replace=False)\n",
    "    sample_1 = rng.choice(index_1, size=num_samples//2, replace=False)\n",
    "    indices = []\n",
    "    # interleave so that both query and support have the same distribution\n",
    "    for ii in range(len(sample_0)):\n",
    "        indices.append(sample_0[ii])\n",
    "        indices.append(sample_1[ii])\n",
    "    return indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils_torch import CustomDataset\n",
    "\n",
    "rng = default_rng()\n",
    "sample_indices = get_balanced_samples(valid_X, valid_y, num_test_samples)\n",
    "tst_x = [valid_X[ii] for ii in sample_indices]\n",
    "tst_y = [valid_y[ii] for ii in sample_indices]\n",
    "\n",
    "valid_set = CustomDataset(tst_x, \n",
    "                          tst_y, \n",
    "                          item_dict, \n",
    "                          pv_items, \n",
    "                          image_dir=image_dir, \n",
    "                          batch_size=batch_size,\n",
    "                          max_len=max_seq_len,\n",
    "                          only_image=not include_text,\n",
    "                          image_embedding_dim=image_embedding_dim,\n",
    "                          image_embedding_file=image_embedding_file,\n",
    "                          text_embedding_file=text_embedding_file,\n",
    "                          number_items_in_batch=150,\n",
    "                          variable_length_input=True,\n",
    "                          text_embedding_dim=text_embedding_dim,\n",
    "                          include_item_categories=include_item_categories,\n",
    "                          image_data=image_data_type,\n",
    "                          input_size=(3, 224, 224),\n",
    "                         )\n",
    "\n",
    "eval_sampler = SequentialSampler(valid_set)\n",
    "eval_dataloader = DataLoader(valid_set,\n",
    "                             sampler=eval_sampler,\n",
    "                             batch_size=256)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss 0.332929: 100%|██████████| 20000/20000 [12:18<00:00, 27.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final loss: 0.333\n",
      "torch.Size([2, 64]) tensor([0., 1.])\n"
     ]
    }
   ],
   "source": [
    "optimizer = AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=learning_rate)\n",
    "global_step = 0\n",
    "num_samples = num_support + num_query\n",
    "\n",
    "for tn in range(num_episodes):\n",
    "    sample_indices = get_balanced_samples(train_X, train_y, num_samples)\n",
    "    tr_x = [train_X[ii] for ii in sample_indices]\n",
    "    tr_y = [train_y[ii] for ii in sample_indices]\n",
    "\n",
    "    train_set = CustomDataset(tr_x, tr_y,\n",
    "                              item_dict, \n",
    "                              pv_items, \n",
    "                              image_dir=image_dir, \n",
    "                              batch_size=num_samples,\n",
    "                              max_len=max_seq_len,\n",
    "                              only_image=not include_text,\n",
    "                              image_embedding_dim=image_embedding_dim,\n",
    "                              image_embedding_file=image_embedding_file,\n",
    "                              text_embedding_file=text_embedding_file,\n",
    "                              number_items_in_batch=150,\n",
    "                              variable_length_input=True,\n",
    "                              text_embedding_dim=text_embedding_dim,\n",
    "                              include_item_categories=include_item_categories,\n",
    "                              image_data=image_data_type,\n",
    "                              input_size=(3, 224, 224),\n",
    "                             )\n",
    "    \n",
    "    all_sampler = RandomSampler(train_set)\n",
    "    all_dataloader = DataLoader(train_set,\n",
    "                                sampler=all_sampler,\n",
    "                                batch_size=num_samples)\n",
    "\n",
    "    batch_x, batch_y = next(iter(all_dataloader))\n",
    "\n",
    "    model.train()\n",
    "    # train for the current task\n",
    "    pbar = tqdm(range(num_train_epochs))\n",
    "#     pbar = tqdm(range(1000))\n",
    "    losses = []\n",
    "    for epoch in pbar:\n",
    "        \n",
    "        batch_d = [x.to(device) for x in batch_x]\n",
    "        outputs = model(batch_d)\n",
    "        model.zero_grad()\n",
    "        loss = proto_loss(outputs, batch_y.to(device), num_support)\n",
    "        pbar.set_description(\"Loss %g\" % loss.item())\n",
    "        losses.append(loss.item())\n",
    "        \n",
    "        # backward pass to get the gradients\n",
    "        loss.backward()\n",
    "\n",
    "        # update\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        global_step += 1\n",
    "    \n",
    "    print(f\"final loss: {loss.item():.3f}\")\n",
    "\n",
    "# get the prototypes from the support set\n",
    "train_outputs = model(batch_d)\n",
    "prototypes, classes = get_prototypes(train_outputs, batch_y, num_support)\n",
    "print(prototypes.shape, classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 4/4 [00:00<00:00,  5.83it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'loss': 2.5297014117240906,\n",
       " 'precision': 0.5099099099099099,\n",
       " 'recall': 0.566,\n",
       " 'f1': 0.5364928909952607,\n",
       " 'auc': 0.504934}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from few_shot_models import evaluate\n",
    "\n",
    "evaluate(model, eval_dataloader, prototypes, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 40/40 [00:06<00:00,  5.91it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'loss': 2.6250036537647246,\n",
       " 'precision': 0.5038532110091744,\n",
       " 'recall': 0.5492,\n",
       " 'f1': 0.5255502392344498,\n",
       " 'auc': 0.4996244}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_set = CustomDataset(valid_X, \n",
    "                          valid_y, \n",
    "                          item_dict, \n",
    "                          pv_items, \n",
    "                          image_dir=image_dir, \n",
    "                          batch_size=batch_size,\n",
    "                          max_len=max_seq_len,\n",
    "                          only_image=not include_text,\n",
    "                          image_embedding_dim=image_embedding_dim,\n",
    "                          image_embedding_file=image_embedding_file,\n",
    "                          text_embedding_file=text_embedding_file,\n",
    "                          number_items_in_batch=150,\n",
    "                          variable_length_input=True,\n",
    "                          text_embedding_dim=text_embedding_dim,\n",
    "                          include_item_categories=include_item_categories,\n",
    "                          image_data=image_data_type,\n",
    "                          input_size=(3, 224, 224),\n",
    "                         )\n",
    "\n",
    "eval_sampler = SequentialSampler(valid_set)\n",
    "eval_dataloader = DataLoader(valid_set,\n",
    "                             sampler=eval_sampler,\n",
    "                             batch_size=256)\n",
    "\n",
    "evaluate(model, eval_dataloader, prototypes, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
