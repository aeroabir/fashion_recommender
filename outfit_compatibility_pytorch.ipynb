{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import json\n",
    "import math\n",
    "import numpy as np\n",
    "import pickle\n",
    "import random\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "\n",
    "from stgnn import Model\n",
    "from stgnn_transformer import Model as Model2\n",
    "# from stgnn_handler import ForecastDataset, train, test\n",
    "# from utils import convert_df_wavelet_input, dl_preprocess_data_test\n",
    "# from utils import rmse, mape, nrmse, nrmse2\n",
    "\n",
    "import torch\n",
    "import torch.utils.data as torch_data\n",
    "import torch.nn as nn\n",
    "from tensorflow.random import set_seed\n",
    "from numpy.random import seed\n",
    "import shap\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = \"/recsys_data/RecSys/fashion/polyvore-dataset/polyvore_outfits\"\n",
    "train_dir = os.path.join(base_dir, \"disjoint\")\n",
    "image_dir = os.path.join(base_dir, \"images\")\n",
    "train_json = \"train.json\"\n",
    "valid_json = \"valid.json\"\n",
    "test_json = \"test.json\"\n",
    "\n",
    "train_file = \"compatibility_train.txt\"\n",
    "valid_file = \"compatibility_valid.txt\"\n",
    "test_file = \"compatibility_test.txt\"\n",
    "item_file = \"polyvore_item_metadata.json\"\n",
    "outfit_file = \"polyvore_outfit_titles.json\"\n",
    "\n",
    "model_type = \"rnn\" #\"set-transformer\"\n",
    "include_text = True\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read all the required files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(train_dir, train_json), 'r') as fr:\n",
    "    train_pos = json.load(fr)\n",
    "    \n",
    "with open(os.path.join(train_dir, valid_json), 'r') as fr:\n",
    "    valid_pos = json.load(fr)\n",
    "    \n",
    "with open(os.path.join(train_dir, test_json), 'r') as fr:\n",
    "    test_pos = json.load(fr)\n",
    "    \n",
    "with open(os.path.join(base_dir, item_file), 'r') as fr:\n",
    "    pv_items = json.load(fr)\n",
    "    \n",
    "with open(os.path.join(base_dir, outfit_file), 'r') as fr:\n",
    "    pv_outfits = json.load(fr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(train_dir, train_file), 'r') as fr:\n",
    "    train_X, train_y = [], []\n",
    "    for line in fr:\n",
    "        elems = line.strip().split()\n",
    "        train_y.append(elems[0])\n",
    "        train_X.append(elems[1:])\n",
    "\n",
    "with open(os.path.join(train_dir, valid_file), 'r') as fr:\n",
    "    valid_X, valid_y = [], []\n",
    "    for line in fr:\n",
    "        elems = line.strip().split()\n",
    "        valid_y.append(elems[0])\n",
    "        valid_X.append(elems[1:])\n",
    "\n",
    "with open(os.path.join(train_dir, test_file), 'r') as fr:\n",
    "    test_X, test_y = [], []\n",
    "    for line in fr:\n",
    "        elems = line.strip().split()\n",
    "        test_y.append(elems[0])\n",
    "        test_X.append(elems[1:])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a dict that maps to real item-id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85935\n",
      "101223\n",
      "175485\n"
     ]
    }
   ],
   "source": [
    "item_dict = {}\n",
    "for ii, outfit in enumerate(train_pos):\n",
    "    items = outfit['items']\n",
    "    mapped = train_X[ii]\n",
    "    item_dict.update({jj:kk['item_id'] for jj, kk in zip(mapped, items)})\n",
    "print(len(item_dict))\n",
    "\n",
    "for ii, outfit in enumerate(valid_pos):\n",
    "    items = outfit['items']\n",
    "    mapped = valid_X[ii]\n",
    "    item_dict.update({jj:kk['item_id'] for jj, kk in zip(mapped, items)})\n",
    "print(len(item_dict))\n",
    "\n",
    "for ii, outfit in enumerate(test_pos):\n",
    "    items = outfit['items']\n",
    "    mapped = test_X[ii]\n",
    "    item_dict.update({jj:kk['item_id'] for jj, kk in zip(mapped, items)})\n",
    "print(len(item_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stgnn_transformer import Model as Model2\n",
    "from stgnn_transformer import BaseTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_set = {}\n",
    "params_set['set1'] = {'order': 4, \n",
    "                      'factor': 3,\n",
    "                      'dropout': 0.05,\n",
    "                      'd_model': 32,\n",
    "                      'num_heads': 4, \n",
    "                      'e_layers': 1, \n",
    "                      'dff': 128,\n",
    "                      'only_transformer': True,\n",
    "                      'use_fft_transformer': True,\n",
    "                      'observe': 'rmse',\n",
    "                      'patience': 15,\n",
    "                      'lr': 1e-05}\n",
    "\n",
    "params_set['set2'] = {'order': 4, \n",
    "                      'factor': 3,\n",
    "                      'dropout': 0.43655976679400943,\n",
    "                      'd_model': 96,\n",
    "                      'num_heads': 4, \n",
    "                      'e_layers': 2, \n",
    "                      'dff': 180,\n",
    "                      'only_transformer': True,\n",
    "                      'use_fft_transformer': True,\n",
    "                      'observe': 'rmse',\n",
    "                      'patience': 15,\n",
    "                      'lr': 1e-05}\n",
    "\n",
    "params_set['set3'] = {'order': 4, \n",
    "                      'factor': 3,\n",
    "                      'dropout': 0.05,\n",
    "                      'd_model': 32,\n",
    "                      'num_heads': 4, \n",
    "                      'e_layers': 1, \n",
    "                      'dff': 128,\n",
    "                      'only_transformer': True,\n",
    "                      'use_fft_transformer': False,\n",
    "                      'observe': 'rmse',\n",
    "                      'patience': 15,\n",
    "                      'lr': 1e-05}\n",
    "\n",
    "params_set['set4'] = {'order': 4, \n",
    "                      'factor': 3,\n",
    "                      'dropout': 0.05,\n",
    "                      'd_model': 32,\n",
    "                      'num_heads': 4, \n",
    "                      'e_layers': 2, \n",
    "                      'dff': 128,\n",
    "                      'only_transformer': True,\n",
    "                      'use_fft_transformer': False,\n",
    "                      'observe': 'rmse',\n",
    "                      'patience': 15,\n",
    "                      'lr': 1e-05}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Trainable Params: 202113\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BaseTransformer(\n",
       "  (image_projector): Linear(in_features=1080, out_features=64, bias=True)\n",
       "  (image_transformer): TransformerEncoder(\n",
       "    (layers): ModuleList(\n",
       "      (0): TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=64, out_features=32, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=32, out_features=64, bias=True)\n",
       "        (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (text_projector): Linear(in_features=768, out_features=64, bias=True)\n",
       "  (text_transformer): TransformerEncoder(\n",
       "    (layers): ModuleList(\n",
       "      (0): TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=64, out_features=32, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=32, out_features=64, bias=True)\n",
       "        (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (rnn): LSTM(128, 32, batch_first=True, bidirectional=True)\n",
       "  (final): Sequential(\n",
       "    (0): Linear(in_features=64, out_features=1, bias=True)\n",
       "    (1): Sigmoid()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "setnum = 'set4'\n",
    "# print(params_set[setnum])\n",
    "model_params = {\n",
    "                'lookback': 16,\n",
    "                'horizon': 1,\n",
    "                'batch_size': 32,\n",
    "                'lr': 1e-04,\n",
    "                'epochs': 100,\n",
    "                'scheduler': None,\n",
    "                'small_epochs': 1,\n",
    "                'output_dir': './',\n",
    "                'norm_method': 'min_max_m1',\n",
    "                'exponential_decay_step':5,\n",
    "                'validate_freq':1,\n",
    "                'early_stop':True,\n",
    "                'device': 'cuda',\n",
    "            }\n",
    "node_cnt = 1280\n",
    "device = model_params['device']\n",
    "\n",
    "# model = Model2(units=node_cnt,\n",
    "#                stack_cnt=1,\n",
    "#                time_step=model_params['lookback'],\n",
    "#                multi_layer=2,\n",
    "#                out_feature=1,\n",
    "#                horizon=model_params['horizon'],\n",
    "#                encoder2='fc',\n",
    "#                order=params_set[setnum]['order'],  # tune parameter\n",
    "#                factor=params_set[setnum]['factor'],\n",
    "#                dropout=params_set[setnum]['dropout'],  # tune parameter, 0.05\n",
    "#                d_model=params_set[setnum]['d_model'],\n",
    "#                n_heads=params_set[setnum]['num_heads'],\n",
    "#                e_layers=params_set[setnum]['e_layers'],\n",
    "#                d_ff=params_set[setnum]['dff'],\n",
    "#                only_transformer=params_set[setnum]['only_transformer'],\n",
    "#                use_fft_transformer=params_set[setnum]['use_fft_transformer'],\n",
    "#              )\n",
    "\n",
    "model = BaseTransformer(1080, 768, \n",
    "                        num_layers=1,\n",
    "                        d_model=64,\n",
    "                        num_heads=1,\n",
    "                        dff=32,\n",
    "                        rate=0.0,\n",
    "                        num_classes=2,\n",
    "                        lstm_dim=32,\n",
    "                        device=device,\n",
    "                        embedding_activation=\"linear\",\n",
    "                        encoder_activation=\"relu\",\n",
    "                        lstm_activation=\"linear\",\n",
    "                        final_activation=\"sigmoid\")\n",
    "\n",
    "total_params = 0\n",
    "for name, parameter in model.named_parameters():\n",
    "    if not parameter.requires_grad:\n",
    "        continue\n",
    "    param = parameter.numel()\n",
    "    total_params += param\n",
    "print(f\"Total Trainable Params: {total_params}\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils_torch import CustomDataset, train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = CustomDataset(train_X, \n",
    "                          train_y, \n",
    "                          item_dict, \n",
    "                          pv_items, \n",
    "                          image_dir, \n",
    "                          batch_size=batch_size, \n",
    "                          only_image=not include_text)\n",
    "valid_set = CustomDataset(valid_X, \n",
    "                          valid_y, \n",
    "                          item_dict, \n",
    "                          pv_items, \n",
    "                          image_dir, \n",
    "                          batch_size=batch_size, \n",
    "                          only_image=not include_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Epoch   0 | time: 35.51s | train_loss 0.6934 | val-ACC 0.5013 | val-AUC 0.5001 ( 0)\n",
      "| Epoch   1 | time: 35.02s | train_loss 0.6932 | val-ACC 0.5007 | val-AUC 0.5042 ( 1)\n",
      "| Epoch   2 | time: 35.01s | train_loss 0.6933 | val-ACC 0.5012 | val-AUC 0.4966 ( 0)\n",
      "| Epoch   3 | time: 35.05s | train_loss 0.6931 | val-ACC 0.4987 | val-AUC 0.4977 ( 1)\n",
      "| Epoch   4 | time: 35.38s | train_loss 0.6934 | val-ACC 0.4992 | val-AUC 0.4988 ( 2)\n",
      "| Epoch   5 | time: 35.53s | train_loss 0.6934 | val-ACC 0.5000 | val-AUC 0.4982 ( 3)\n",
      "| Epoch   6 | time: 34.91s | train_loss 0.6933 | val-ACC 0.5000 | val-AUC 0.5035 ( 4)\n",
      "| Epoch   7 | time: 35.53s | train_loss 0.6934 | val-ACC 0.4995 | val-AUC 0.4995 ( 5)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-f3bfb03ca8da>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'cuda'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/recsys_data/RecSys/fashion/utils_torch.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, train_set, valid_set, device, **kwargs)\u001b[0m\n\u001b[1;32m    181\u001b[0m             \u001b[0;31m# sys.exit()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m             \u001b[0mcnt\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 183\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    184\u001b[0m             \u001b[0mmy_optim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m             \u001b[0mloss_total\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/py37_tensorflow/lib/python3.7/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    305\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 307\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/py37_tensorflow/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    154\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    155\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train(model, train_set, valid_set, device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import flash\n",
    "from flash.core.data.utils import download_data\n",
    "from flash.image import ImageClassificationData, ImageEmbedder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
